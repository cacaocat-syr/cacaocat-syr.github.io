# Blog 2 [2.14.2020]

### Python: Concurrency Introduction

Most scripts or programs that we produce can take a long time to complete a task. Most of the time we loop through a large file or a large list repeating the same task. Concurrency can help reduce the amount of time it takes to complete a repetitive task. Consider the below code for example:

Code:
```
import http.client
import socket
from datetime import datetime


def get_it(url):
    try:
        # always set a timeout when you connect to an external server
        connection = http.client.HTTPSConnection(url, timeout=2)

        connection.request("GET", "/")

        response = connection.getresponse()

        return response.read()
    except socket.timeout:
        # in a real world scenario you would probably do stuff if the
        # socket goes into timeout
        pass

urls = [
    "www.google.com",
    "www.youtube.com",
    "www.wikipedia.org",
    "www.reddit.com",
    "www.httpbin.org"
] * 10

start = datetime.now()
for url in urls:
    stuff = get_it(url)
    print(url + ": " + str(stuff))
end = datetime.now()

print(end-start)
```
Here we are simply using the http.client module to make GET requests to a list of URLs which is repeated 10 times. We are also using the datetime module to time our code. This code is executed linearly, doing the same task one at a time with each URL.

Running this code, below are the times that it took to run:

- t1 = 35 seconds
- t2 = 35 seconds
- t3 = 34 seconds

On average, it takes about 35 seconds to complete this script. However if we implement concurrency we could greatly improve the efficiency of this script.
```
from concurrent.futures import ThreadPoolExecutor as PoolExecutor
import http.client
import socket
from datetime import datetime


def get_it(url):
    try:
        # always set a timeout when you connect to an external server
        connection = http.client.HTTPSConnection(url, timeout=2)

        connection.request("GET", "/")

        response = connection.getresponse()

        print(response.read())
    except socket.timeout:
        # in a real world scenario you would probably do stuff if the
        # socket goes into timeout
        pass

urls = [
    "www.google.com",
    "www.youtube.com",
    "www.wikipedia.org",
    "www.reddit.com",
    "www.httpbin.org"
] * 10

start = datetime.now()


with PoolExecutor(max_workers=4) as executor:
    for x in executor.map(get_it, urls):
        pass


end = datetime.now()

print(end-start)
```
Here we completely replaced the for loop with...
```
with PoolExecutor(max_workers=4) as executor:
    for x in executor.map(get_it, urls):
        pass
```
... which is the most important part.

Here we are declaring 4 workers in the executor. ```executor.map(get_it, urls)``` means with ```urls``` do ```get_it``` function. Each of the 4 workers will take a url from ```urls``` and do the ```get_it``` function. Meaning instead of running 1 instance of the ```get_it``` function at a time, we will be running 4 at once. Below are the new times it took to run this script:

- t1 = 11.47 seconds
- t2 = 11.72 seconds
- t3 = 11.36 seconds

One thing to note is that concurrency should only be used when order doesn't matter. Otherwise you might run into problems with race conditions.

### References
- https://realpython.com/python-concurrency/
- https://dev.to/rhymes/how-to-make-python-code-concurrent-with-3-lines-of-code-2fpe
